{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "The fundamental core of Survey-Enhance is the idea of measuring survey accuracy/usefulness in a singular value that packs together lots of individual targets we're concerned about: everything from tax to benefits to demographics. We call this value the loss. The loss is a measure of how far away the survey is from the truth. It's a single number that we can use to compare different survey designs, and to measure the impact of different survey enhancements.\n",
    "\n",
    "We need to build this ourselves for any given survey, trying to be as neutral as possible. There's strength in numbers: I've incorporated as many different statistics as are readily available for the UK, but of course the way I've constructed the loss function is the most vulnerable part of the pipeline to arbitrary assumptions. I followed the following principles:\n",
    "* Put demographics into one bin, and financial statistics into another, then normalise them and weight them equally.\n",
    "* Within those bins, weight by size (e.g. \"Income Tax statistics\" should be weighted 200:40 to \"Universal Credit statistics\", because Income Tax revenue is £200bn and Universal Credit spending is £40bn).\n",
    "\n",
    "So what does this look like? The following code runs the loss function on the 2019-20 FRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FRS: 1.0\n"
     ]
    }
   ],
   "source": [
    "from loss.loss import Loss, calibration_parameters\n",
    "from datasets.frs import FRS_2019_20\n",
    "from datasets.output_dataset import OutputDataset\n",
    "import torch\n",
    "\n",
    "original_frs = OutputDataset.from_dataset(FRS_2019_20, 2019, 2022)()\n",
    "\n",
    "loss = Loss(\n",
    "    original_frs,\n",
    "    calibration_parameters(f\"2022-01-01\"),\n",
    "    static_dataset=False,\n",
    ")\n",
    "\n",
    "frs_loss = loss(\n",
    "    torch.tensor(original_frs.household.household_weight.values), original_frs\n",
    ")\n",
    "\n",
    "print(f\"Original FRS: {frs_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which isn't too exciting, because it's normalised to 1.0 (deliberately). The value of the loss is fundamentally difficult to understand, because we don't really think of accuracy as a single number. But we can get some level of intuition by changing the survey up a bit and seeing how the loss changes. For example: what if everyone in the FRS lied and said they had no pension income? How much less accurate would the survey be? We have some rough subjective feeling for this, so we can see what the loss function says and calibrate our mental model of accuracy to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRS with no pension income: 6.552893370459233\n"
     ]
    }
   ],
   "source": [
    "class FRS_2019_20_with_no_pension_income(FRS_2019_20):\n",
    "    name = \"FRS_2019_20_with_no_pension_income\"\n",
    "    file_path = (\n",
    "        FRS_2019_20.file_path.parent / \"frs_2019_20_with_no_pension_income.h5\"\n",
    "    )\n",
    "\n",
    "    def generate(self):\n",
    "        super().generate()\n",
    "        pension_income = self.load(\"pension_income\")\n",
    "        self.save(\"pension_income\", pension_income * 0)\n",
    "\n",
    "\n",
    "frs_with_no_pension_income = OutputDataset.from_dataset(\n",
    "    FRS_2019_20_with_no_pension_income, 2019, 2022\n",
    ")()\n",
    "\n",
    "frs_with_no_pension_income_loss = loss(\n",
    "    torch.tensor(frs_with_no_pension_income.household.household_weight.values),\n",
    "    frs_with_no_pension_income,\n",
    ")\n",
    "\n",
    "print(f\"FRS with no pension income: {frs_with_no_pension_income_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the loss jumped to around 6. Why is that? We can check the loss function to see what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss.Programs:\n",
      "  1_loss: 12.105786740918466\n",
      "  2_weight: 1\n",
      "  3_children:\n",
      "    Loss.Programs.ChildTaxCredit:\n",
      "      1_loss: 0.996940178801625\n",
      "      2_weight: 13.875\n",
      "      3_children:\n",
      "        Loss.Programs.ChildTaxCredit.child_tax_credit_budgetary_impact:\n",
      "          1_loss: 0.9938803576032499\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "    Loss.Programs.HousingBenefit:\n",
      "      1_loss: 0.9403234612227025\n",
      "      2_weight: 15.894\n",
      "      3_children:\n",
      "        Loss.Programs.HousingBenefit.housing_benefit_budgetary_impact:\n",
      "          1_loss: 0.9288759046814592\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "        Loss.Programs.HousingBenefit.housing_benefit_participants:\n",
      "          1_loss: 0.9517710177639459\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "    Loss.Programs.IncomeSupport:\n",
      "      1_loss: 1.0020913645503176\n",
      "      2_weight: 0.67\n",
      "      3_children:\n",
      "        Loss.Programs.IncomeSupport.income_support_budgetary_impact:\n",
      "          1_loss: 1.0041827291006353\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "    Loss.Programs.IncomeTax:\n",
      "      1_loss: 6.670099754003721\n",
      "      2_weight: 200\n",
      "      3_children:\n",
      "        Loss.Programs.IncomeTax.IncomeTaxBudgetaryImpact:\n",
      "          1_loss: 1.4495706264860806\n",
      "          2_weight: 1.0\n",
      "          3_children: {}\n",
      "        Loss.Programs.IncomeTax.IncomeTaxParticipants:\n",
      "          1_loss: 11.890628881521362\n",
      "          2_weight: 1.0\n",
      "          3_children: {}\n",
      "    Loss.Programs.PensionCredit:\n",
      "      1_loss: 11.28762767221413\n",
      "      2_weight: 4.466\n",
      "      3_children:\n",
      "        Loss.Programs.PensionCredit.pension_credit_budgetary_impact:\n",
      "          1_loss: 8.461916220883241\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "        Loss.Programs.PensionCredit.pension_credit_participants:\n",
      "          1_loss: 14.11333912354502\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "    Loss.Programs.PensionIncome:\n",
      "      1_loss: 164.49260630151875\n",
      "      2_weight: 107.3\n",
      "      3_children:\n",
      "        Loss.Programs.PensionIncome.pension_income_budgetary_impact:\n",
      "          1_loss: 294.1710747907839\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "        Loss.Programs.PensionIncome.pension_income_participants:\n",
      "          1_loss: 34.8141378122536\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "    Loss.Programs.UniversalCredit:\n",
      "      1_loss: 6.0452700739001335\n",
      "      2_weight: 43.657\n",
      "      3_children:\n",
      "        Loss.Programs.UniversalCredit.universal_credit_budgetary_impact:\n",
      "          1_loss: 0.5515091927126515\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "        Loss.Programs.UniversalCredit.universal_credit_participants:\n",
      "          1_loss: 11.539030955087615\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "    Loss.Programs.WorkingTaxCredit:\n",
      "      1_loss: 0.980615006865072\n",
      "      2_weight: 3.825\n",
      "      3_children:\n",
      "        Loss.Programs.WorkingTaxCredit.working_tax_credit_budgetary_impact:\n",
      "          1_loss: 0.9803294558918838\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "        Loss.Programs.WorkingTaxCredit.working_tax_credit_participants:\n",
      "          1_loss: 0.9809005578382601\n",
      "          2_weight: 1\n",
      "          3_children: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor(\n",
    "    frs_with_no_pension_income.household.household_weight.values\n",
    ")\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "print(yaml.dump(loss.computation_tree(weights, frs_with_no_pension_income)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this is showing us is which parts of the loss function are most sensitive to the change we made. We can see that the biggest single loss change came from the pension income category (this makes sense- zeroing out pension incomes makes it very difficult to hit total pension income statistics!). But there were lots of knock-on effects on other categories, too: total taxpayer count statistics were significantly off after the change (likely because a lot of people pay tax solely because of their pension income, since the State Pension on its own is not enough to push you into the tax system).\n",
    "\n",
    "As another sanity check, let's see what the loss is if we just toned down the pension income by 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRS with 10% less pension income: 1.0334495620378865\n"
     ]
    }
   ],
   "source": [
    "class FRS_2019_20_with_too_little_pension_income(FRS_2019_20):\n",
    "    name = \"FRS_2019_20_with_too_little_pension_income\"\n",
    "    file_path = (\n",
    "        FRS_2019_20.file_path.parent\n",
    "        / \"frs_2019_20_with_too_little_pension_income.h5\"\n",
    "    )\n",
    "\n",
    "    def generate(self):\n",
    "        super().generate()\n",
    "        pension_income = self.load(\"pension_income\")\n",
    "        self.save(\"pension_income\", pension_income * 0.9)\n",
    "\n",
    "\n",
    "frs_with_too_little_pension_income = OutputDataset.from_dataset(\n",
    "    FRS_2019_20_with_too_little_pension_income, 2019, 2022\n",
    ")()\n",
    "\n",
    "frs_with_too_little_pension_income_loss = loss(\n",
    "    torch.tensor(\n",
    "        frs_with_too_little_pension_income.household.household_weight.values\n",
    "    ),\n",
    "    frs_with_too_little_pension_income,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"FRS with 10% less pension income: {frs_with_too_little_pension_income_loss}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which is a much smaller change, and the loss function is much more stable- for lots of little reasons, like the fact that we didn't cross a lot of people over boundaries between tax bands, etc. etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
